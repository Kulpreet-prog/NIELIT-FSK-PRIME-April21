{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kulpreet-prog/NIELIT-FSK-PRIME-April21/blob/main/copy_of_task_7__introduction_to_natural_language_text_processing_ai_bootcamp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 7: Introduction to Natural Language (Text) Processing"
      ],
      "metadata": {
        "id": "L982FdumDCVn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1: Setup and Sample Dataset"
      ],
      "metadata": {
        "id": "gk9AwRFXDO6n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 1**: Import Libraries and Sample Data\n",
        "*Instruction*: Import the necessary libraries and define a sample dataset for sentiment classification."
      ],
      "metadata": {
        "id": "tG2LLFb4DSrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Sample Dataset for Sentiment Classification (positive/negative)\n",
        "data = {\n",
        "    'text': [\n",
        "        'I love this product, it is amazing!',\n",
        "        'Worst purchase I have ever made.',\n",
        "        'Highly recommend, this is a great buy.',\n",
        "        'Not worth the price, very disappointed.',\n",
        "        'I am so happy with my new purchase.',\n",
        "        'Very bad experience, would not buy again.',\n",
        "        'Absolutely fantastic! Best decision ever.',\n",
        "        'Terrible, do not buy this product.',\n",
        "        'Love it! Will buy again.',\n",
        "        'Worst customer service ever.'\n",
        "    ],\n",
        "    'sentiment': [\n",
        "        'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Convert the data to a pandas DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "G6YtbgenDSWH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c15435e1-e2c7-4867-bd25-ee6784c4977d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                      text sentiment\n",
            "0      I love this product, it is amazing!  positive\n",
            "1         Worst purchase I have ever made.  negative\n",
            "2   Highly recommend, this is a great buy.  positive\n",
            "3  Not worth the price, very disappointed.  negative\n",
            "4      I am so happy with my new purchase.  positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2: Text Preprocessing"
      ],
      "metadata": {
        "id": "03CKwCBtDzRL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 2**: Clean the Text\n",
        "\n",
        "*Instruction*: Lowercase the text, remove punctuation, stopwords, and tokenize the sentences.\n"
      ],
      "metadata": {
        "id": "oh1W_9m5DuzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Download stopwords only\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Sample dataset\n",
        "data = {\n",
        "    'text': [\n",
        "        'I love this product! It is amazing.',\n",
        "        'This is the worst experience ever.',\n",
        "        'Absolutely fantastic service!',\n",
        "        'I am not happy with the quality.',\n",
        "        'Best purchase I have made!'\n",
        "    ]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Define stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Text cleaning function\n",
        "def clean_text(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Simple tokenization by split\n",
        "    tokens = text.split()\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Apply cleaning to the dataset\n",
        "df['cleaned_text'] = df['text'].apply(clean_text)\n",
        "\n",
        "# Display the cleaned text\n",
        "print(df[['text', 'cleaned_text']])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SQTsWR6GDn6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c31fa061-8d2a-445c-8113-679ea921d2f1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                  text                      cleaned_text\n",
            "0  I love this product! It is amazing.          [love, product, amazing]\n",
            "1   This is the worst experience ever.         [worst, experience, ever]\n",
            "2        Absolutely fantastic service!  [absolutely, fantastic, service]\n",
            "3     I am not happy with the quality.                  [happy, quality]\n",
            "4           Best purchase I have made!            [best, purchase, made]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 3: Text Vectorization"
      ],
      "metadata": {
        "id": "mVV1BgZvEE3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 3**: Convert Text to Numerical Features\n",
        "\n",
        "*Instruction*: Use both Bag of Words and TF-IDF vectorization to convert the cleaned text.\n"
      ],
      "metadata": {
        "id": "opUK7Z7LEIr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# First, join tokens back into sentences because vectorizers expect string input\n",
        "df['cleaned_text_joined'] = df['cleaned_text'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Bag of Words (BoW)\n",
        "bow_vectorizer = CountVectorizer()\n",
        "X_bow = bow_vectorizer.fit_transform(df['cleaned_text_joined'])\n",
        "\n",
        "# TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df['cleaned_text_joined'])\n",
        "\n",
        "# Check the shape of matrices\n",
        "print(\"Bag of Words Shape:\", X_bow.shape)\n",
        "print(\"TF-IDF Shape:\", X_tfidf.shape)\n"
      ],
      "metadata": {
        "id": "UW3FMdjQEEl3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ee78756-7105-4e8e-fc5d-8f7a35f81c7d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words Shape: (5, 14)\n",
            "TF-IDF Shape: (5, 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 4: Train a Classifier"
      ],
      "metadata": {
        "id": "GNO0DPi3EpgF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 4**: Sentiment Classification with Naive Bayes\n",
        "\n",
        "*Instruction*: Split the dataset, train a classifier using both feature sets, and evaluate the performance."
      ],
      "metadata": {
        "id": "W74DNGaJEtdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())\n",
        "# Add sentiment manually\n",
        "df['sentiment'] = ['positive', 'negative', 'positive', 'negative', 'positive']\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Split the data\n",
        "X_train_bow, X_test_bow, y_train, y_test = train_test_split(X_bow, df['sentiment'], test_size=0.2, random_state=42)\n",
        "X_train_tfidf, X_test_tfidf, _, _ = train_test_split(X_tfidf, df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Model 1: Bag of Words\n",
        "model_bow = MultinomialNB()\n",
        "model_bow.fit(X_train_bow, y_train)\n",
        "y_pred_bow = model_bow.predict(X_test_bow)\n",
        "\n",
        "# Model 2: TF-IDF\n",
        "model_tfidf = MultinomialNB()\n",
        "model_tfidf.fit(X_train_tfidf, y_train)\n",
        "y_pred_tfidf = model_tfidf.predict(X_test_tfidf)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Accuracy using Bag of Words:\", accuracy_score(y_test, y_pred_bow))\n",
        "print(\"Accuracy using TF-IDF:\", accuracy_score(y_test, y_pred_tfidf))\n",
        "\n"
      ],
      "metadata": {
        "id": "aM8iWEAXEOmE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9788b6c-903e-41c1-e8f7-d57e044cd80e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                  text                      cleaned_text  \\\n",
            "0  I love this product! It is amazing.          [love, product, amazing]   \n",
            "1   This is the worst experience ever.         [worst, experience, ever]   \n",
            "2        Absolutely fantastic service!  [absolutely, fantastic, service]   \n",
            "3     I am not happy with the quality.                  [happy, quality]   \n",
            "4           Best purchase I have made!            [best, purchase, made]   \n",
            "\n",
            "            cleaned_text_joined  \n",
            "0          love product amazing  \n",
            "1         worst experience ever  \n",
            "2  absolutely fantastic service  \n",
            "3                 happy quality  \n",
            "4            best purchase made  \n",
            "Accuracy using Bag of Words: 0.0\n",
            "Accuracy using TF-IDF: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 5: Mini Challenge â€“ Classify Your Own Text"
      ],
      "metadata": {
        "id": "yFxPFagsE9mS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 5**:  User Input Prediction\n",
        "\n",
        "*Instruction*: Write a function that allows the user to enter a text and receive a prediction from the trained model.\n"
      ],
      "metadata": {
        "id": "IZwIOzHXFD1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')  # Download for tokenization\n",
        "\n",
        "# Sample dataset\n",
        "data = {\n",
        "    'text': [\n",
        "        'I love this product! It is amazing.',\n",
        "        'This is the worst experience ever.',\n",
        "        'Absolutely fantastic service!',\n",
        "        'I am not happy with the quality.',\n",
        "        'Best purchase I have made!'\n",
        "    ],\n",
        "    'sentiment': ['positive', 'negative', 'positive', 'negative', 'positive']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Define stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Text cleaning function (same as before)\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "df['cleaned_text'] = df['text'].apply(clean_text)\n",
        "df['cleaned_text_joined'] = df['cleaned_text'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Bag of Words (BoW) and TF-IDF vectorization\n",
        "bow_vectorizer = CountVectorizer()\n",
        "X_bow = bow_vectorizer.fit_transform(df['cleaned_text_joined'])\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df['cleaned_text_joined'])\n",
        "\n",
        "# Split the data (same as before)\n",
        "X_train_bow, X_test_bow, y_train, y_test = train_test_split(X_bow, df['sentiment'], test_size=0.2, random_state=42)\n",
        "X_train_tfidf, X_test_tfidf, _, _ = train_test_split(X_tfidf, df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the models (same as before)\n",
        "model_bow = MultinomialNB()\n",
        "model_bow.fit(X_train_bow, y_train)\n",
        "\n",
        "model_tfidf = MultinomialNB()\n",
        "model_tfidf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Function to predict sentiment (same as before)\n",
        "def predict_sentiment(user_text, model, vectorizer, stop_words):\n",
        "    user_text = user_text.lower()\n",
        "    user_text = user_text.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = word_tokenize(user_text)\n",
        "    cleaned_tokens = [word for word in tokens if word not in stop_words]\n",
        "    if not cleaned_tokens:\n",
        "        return \"Neutral\"\n",
        "    cleaned_text = ' '.join(cleaned_tokens)\n",
        "    vectorized_text = vectorizer.transform([cleaned_text])\n",
        "    prediction = model.predict(vectorized_text)\n",
        "    return prediction[0]\n",
        "\n",
        "# Get user input and predict sentiment\n",
        "user_input = input(\"Enter your text for sentiment prediction: \")\n",
        "\n",
        "# Predict using BoW and TF-IDF\n",
        "predicted_sentiment_bow = predict_sentiment(user_input, model_bow, bow_vectorizer, stop_words)\n",
        "predicted_sentiment_tfidf = predict_sentiment(user_input, model_tfidf, tfidf_vectorizer, stop_words)\n",
        "\n",
        "# Print predictions\n",
        "print(\"Predicted Sentiment (Bag of Words):\", predicted_sentiment_bow)\n",
        "print(\"Predicted Sentiment (TF-IDF):\", predicted_sentiment_tfidf)\n",
        "\n"
      ],
      "metadata": {
        "id": "VpUFTR1JFDWk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74691435-8ec0-4128-c005-29c981bfe2d6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your text for sentiment prediction:     'Love it! Will buy again.'\n",
            "Predicted Sentiment (Bag of Words): positive\n",
            "Predicted Sentiment (TF-IDF): positive\n"
          ]
        }
      ]
    }
  ]
}